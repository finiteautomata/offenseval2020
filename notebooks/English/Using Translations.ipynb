{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from datetime import datetime\n",
    "import fire\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AdamW, BertForSequenceClassification, BertTokenizer,\n",
    "    get_constant_schedule_with_warmup\n",
    ")\n",
    "import csv\n",
    "\n",
    "from offenseval.nn import (\n",
    "    Tokenizer,\n",
    "    train, evaluate, train_cycle, save_model, load_model, evaluate_dataset\n",
    ")\n",
    "from offenseval.datasets import datasets\n",
    "\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.max_colwidth = 300\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "\n",
    "arabic_train = pd.read_table(\n",
    "    \"../../data/Arabic/offenseval-ar-training-v1.tsv\", \n",
    "    index_col=0,\n",
    "    quoting=csv.QUOTE_NONE\n",
    ")\n",
    "\n",
    "dfs = []\n",
    "for file in glob.glob(\"../../data/translations/Arabic/*training*.json\"):\n",
    "    translation_df = pd.read_json(file)\n",
    "    translation_df.set_index(\"id\", inplace=True)\n",
    "    text_col = translation_df.columns[0]\n",
    "    translation_df[\"subtask_a\"] = arabic_train[\"subtask_a\"]\n",
    "    translation_df.reset_index(inplace=True)\n",
    "    translation_df.rename(columns={text_col: \"tweet\", \"id\": \"original_id\"}, inplace=True)\n",
    "    \n",
    "    dfs.append(translation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_df_train = pd.concat(dfs)\n",
    "\n",
    "arabic_df_train.index = range(1, 1+len(arabic_df_train))\n",
    "\n",
    "arabic_df_train.to_csv(\"../../data/translations/Arabic/train.tsv\", sep=\"\\t\", index_label=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turkish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "\n",
    "turkish_train = pd.read_table(\n",
    "    \"../../data/Turkish/train.tsv\", \n",
    "    index_col=0,\n",
    "    quoting=csv.QUOTE_NONE\n",
    ")\n",
    "\n",
    "dfs = []\n",
    "for file in glob.glob(\"../../data/translations/Turkish/*training*.json\"):\n",
    "    translation_df = pd.read_json(file)\n",
    "    translation_df.set_index(\"id\", inplace=True)\n",
    "    # Usar solamente los que están en train\n",
    "    translation_df = translation_df.loc[turkish_train.index]\n",
    "    text_col = translation_df.columns[0]\n",
    "    translation_df[\"subtask_a\"] = turkish_train[\"subtask_a\"]\n",
    "    \n",
    "    translation_df.reset_index(inplace=True)\n",
    "    translation_df.rename(columns={text_col: \"tweet\", \"id\": \"original_id\"}, inplace=True)\n",
    "    \n",
    "    dfs.append(translation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_df_trans = pd.concat(dfs)\n",
    "\n",
    "turkish_df_trans.index = range(1, 1+len(turkish_df_trans))\n",
    "\n",
    "turkish_df_trans.to_csv(\"../../data/translations/Turkish/train.tsv\", sep=\"\\t\", index_label=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greek "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "\n",
    "greek_train = pd.read_table(\n",
    "    \"../../data/Greek/train.tsv\", \n",
    "    index_col=0,\n",
    "    quoting=csv.QUOTE_NONE\n",
    ")\n",
    "\n",
    "dfs = []\n",
    "for file in glob.glob(\"../../data/translations/Greek/*training*.json\"):\n",
    "    translation_df = pd.read_json(file)\n",
    "    translation_df.set_index(\"id\", inplace=True)\n",
    "    # Usar solamente los que están en train\n",
    "    translation_df = translation_df.loc[greek_train.index]\n",
    "    text_col = translation_df.columns[0]\n",
    "    translation_df[\"subtask_a\"] = greek_train[\"subtask_a\"]\n",
    "    \n",
    "    translation_df.reset_index(inplace=True)\n",
    "    translation_df.rename(columns={text_col: \"tweet\", \"id\": \"original_id\"}, inplace=True)\n",
    "    \n",
    "    dfs.append(translation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_df_trans = pd.concat(dfs)\n",
    "\n",
    "greek_df_trans.index = range(1, 1+len(greek_df_trans))\n",
    "\n",
    "greek_df_trans.to_csv(\"../../data/translations/Greek/train.tsv\", sep=\"\\t\", index_label=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Danish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "\n",
    "danish_train = pd.read_table(\n",
    "    \"../../data/Danish/train.tsv\", \n",
    "    index_col=0,\n",
    "    quoting=csv.QUOTE_NONE\n",
    ")\n",
    "\n",
    "dfs = []\n",
    "for file in glob.glob(\"../../data/translations/Danish/*training*.json\"):\n",
    "    translation_df = pd.read_json(file)\n",
    "    translation_df.set_index(\"id\", inplace=True)\n",
    "    # Usar solamente los que están en train\n",
    "    translation_df = translation_df.loc[danish_train.index]\n",
    "    text_col = translation_df.columns[0]\n",
    "    translation_df[\"subtask_a\"] = danish_train[\"subtask_a\"]\n",
    "    \n",
    "    translation_df.reset_index(inplace=True)\n",
    "    translation_df.rename(columns={text_col: \"tweet\", \"id\": \"original_id\"}, inplace=True)\n",
    "    \n",
    "    dfs.append(translation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "danish_df_trans = pd.concat(dfs)\n",
    "\n",
    "danish_df_trans.index = range(1, 1+len(danish_df_trans))\n",
    "\n",
    "danish_df_trans.to_csv(\"../../data/translations/Danish/train.tsv\", \n",
    "                       sep=\"\\t\", index_label=\"id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
