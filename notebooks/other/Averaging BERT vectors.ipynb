{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Averaging\n",
    "\n",
    "We see in this notebook how can we calculate the average representation of each contextualized embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from datetime import datetime\n",
    "import fire\n",
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AdamW, BertTokenizer, BertModel\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first tokenize the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'proof']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This is a proof\"\n",
    "\n",
    "sent = tokenizer.tokenize(sentence)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, convert it to ids. Also, we create a tensor with shape `(1, sent len)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.LongTensor(tokenizer.encode(sentence)).view(1, -1)\n",
    "\n",
    "inp.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'this', 'is', 'a', 'proof', '[SEP]']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden, pooled = bert(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using iterator and lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.data as data\n",
    "\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token  = tokenizer.sep_token\n",
    "pad_token  = tokenizer.pad_token\n",
    "unk_token  = tokenizer.unk_token\n",
    "\n",
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx  = tokenizer.sep_token_id\n",
    "pad_token_idx  = tokenizer.pad_token_id\n",
    "unk_token_idx  = tokenizer.unk_token_id\n",
    "\n",
    "TEXT = data.Field(\n",
    "    tokenize=tokenizer.tokenize,\n",
    "    include_lengths = True,\n",
    "    use_vocab=False,\n",
    "    batch_first = True,\n",
    "    preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "    init_token = init_token_idx,\n",
    "    eos_token = eos_token_idx,\n",
    "    pad_token = pad_token_idx,\n",
    "    unk_token = unk_token_idx\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train instances: 908\n"
     ]
    }
   ],
   "source": [
    "ID = data.Field(sequential=False, use_vocab=False)\n",
    "# All these arguments are because these are really floats\n",
    "# See https://github.com/pytorch/text/issues/78#issuecomment-541203609\n",
    "AVG = data.LabelField(dtype = torch.float, use_vocab=False, preprocessing=float)\n",
    "STD = data.LabelField(dtype = torch.float, use_vocab=False, preprocessing=float)\n",
    "SUBTASK_A = data.LabelField()\n",
    "\n",
    "train_dataset = data.TabularDataset(\n",
    "    \"../../data/English/task_a_distant.xsmall.tsv\",\n",
    "    format=\"tsv\", skip_header=True,\n",
    "    fields=[(\"id\", ID), (\"text\", TEXT), (\"avg\", AVG), (\"std\", STD)],\n",
    ")\n",
    "\n",
    "print(f\"Train instances: {len(train_dataset)}\")\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_it = data.BucketIterator(\n",
    "    train_dataset, batch_size=BATCH_SIZE, device=device,\n",
    "    sort_key = lambda x: len(x.text), sort_within_batch = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_it))\n",
    "\n",
    "text, lens = batch.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '@', 'user', 'i', 'don', '[UNK]', 't', 'think', 'you', 'can', 'throw', '30', '[SEP]']\n",
      "tensor(13, device='cuda:0')\n",
      "['[CLS]', '@', 'user', 'when', 'he', 'hits', 'that', 'bong', 'and', 'almost', 'dies', '[UNK]', '[SEP]']\n",
      "tensor(13, device='cuda:0')\n",
      "['[CLS]', '@', 'user', 'wealth', 'is', 'measured', 'different', '##ly', 'among', 'people', 'bro', '[UNK]', '[SEP]']\n",
      "tensor(13, device='cuda:0')\n",
      "['[CLS]', 'this', 'head', '##ache', 'can', 'absolute', '##ly', 'fuck', '##in', 'po', '##ke', 'it', '[SEP]']\n",
      "tensor(13, device='cuda:0')\n",
      "['[CLS]', 'september', '3', 'first', 'day', 'of', 'school', '[UNK]', '#', 'pre', '##k', '[UNK]', '[SEP]']\n",
      "tensor(13, device='cuda:0')\n",
      "['[CLS]', '@', 'user', '@', 'user', 'same', 'energy', 'with', 'the', 'pair', 'phone', '.', '[SEP]']\n",
      "tensor(13, device='cuda:0')\n",
      "['[CLS]', 'rt', 'if', 'ur', 'dick', 'is', 'the', 'same', 'length', 'as', 'your', 'height', '[SEP]']\n",
      "tensor(13, device='cuda:0')\n",
      "['[CLS]', '@', 'user', 'you', 'always', 'come', 'through', '.', 'thanks', 'for', 'voting', '.', '[SEP]']\n",
      "tensor(13, device='cuda:0')\n",
      "['[CLS]', 'had', 'one', 'of', 'the', 'best', 'weekend', '##s', 'of', 'my', 'life', '[UNK]', '[SEP]']\n",
      "tensor(13, device='cuda:0')\n",
      "['[CLS]', 'u', 'were', 'the', 'one', 'that', '[UNK]', 's', 'what', 'i', 'told', 'myself', '[SEP]']\n",
      "tensor(13, device='cuda:0')\n",
      "['[CLS]', 'but', 'that', 'was', 'just', 'a', 'dream', '.', '.', 'just', 'a', 'dream', '[SEP]']\n",
      "tensor(13, device='cuda:0')\n",
      "['[CLS]', 'if', 'anyone', 'would', 'like', 'to', 'send', 'me', '1', '##k', 'lm', '##k', '[SEP]']\n",
      "tensor(13, device='cuda:0')\n",
      "['[CLS]', '@', 'user', '@', 'user', '@', 'user', 'and', 'it', '[UNK]', 's', 'tiny', '[SEP]']\n",
      "tensor(13, device='cuda:0')\n",
      "['[CLS]', '@', 'user', 'i', \"'\", 'm', 'doing', 'the', 'test', 'later', 'oo', '##p', '[SEP]']\n",
      "tensor(13, device='cuda:0')\n",
      "['[CLS]', 'but', 'she', '[UNK]', 's', 'sha', '##king', 'the', 'cat', '##aco', '##mb', '##s', '[SEP]']\n",
      "tensor(13, device='cuda:0')\n",
      "['[CLS]', '@', 'user', '@', 'user', '@', 'user', 'her', 'last', 'hit', '[UNK]', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', '@', 'user', 'of', 'course', '!', 'no', 'big', '##gie', '!', '[UNK]', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', 'bow', 'and', 'gli', '##mmer', 'are', 'the', 'best', ':', '(', '(', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', 'soo', '##o', 'many', 'hot', 'girl', 'summers', 'to', 'keep', 'track', 'of', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', 'td', 'beverly', 'and', 'the', 'pat', 'is', 'good', '7', '-', '7', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', '@', 'user', '@', 'user', 'not', 'a', 'fan', 'of', 'that', 'term', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', '@', 'user', 'why', 'was', 'that', 'guy', 'sent', 'to', 'jail', '?', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', '@', 'user', 'aw', '##w', 'that', 'is', 'so', 'cut', '##e', '[UNK]', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', 'ha', '!', 'i', \"'\", 'm', 'stronger', 'than', 'i', 'look', '.', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', 'joe', 'dese', '##rve', '##s', 'the', 'world', ',', 'pass', 'it', 'on', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', '@', 'user', 'wow', 'thanks', 'ais', '##ha', 'i', 'love', 'you', 'too', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', '@', 'user', 'was', 'anyone', 'on', 'their', 'knee', 'for', 'that', '?', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', '@', 'user', 'thank', 'you', 'for', 'the', 'sho', '##uto', '##ut', '!', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', 'thank', 'you', 'god', 'for', 'keeping', 'things', 'running', 'smooth', '##ly', '[UNK]', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', '@', 'user', 'ot', '##w', 'i', 'got', 'the', 'sna', '##cks', 'too', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', 'and', 'they', 'post', '##pone', 'this', 'just', 'wasn', \"'\", 't', 'the', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n",
      "['[CLS]', 'really', 'want', 'to', 'do', 'a', 'trip', 'to', 'mexico', 'soon', '[UNK]', '[SEP]', '[PAD]']\n",
      "tensor(12, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    print(tokenizer.convert_ids_to_tokens(text[i]))\n",
    "    print(lens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.to(device)\n",
    "bert = bert.to(device)\n",
    "hidden, pooled = bert(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0406, -0.3458,  0.3251,  ...,  0.0376,  0.2888, -0.2381],\n",
       "        [ 0.0726, -0.1260,  0.3359,  ..., -0.2967,  0.2420, -0.1426],\n",
       "        [-0.1861, -0.0908,  0.1032,  ...,  0.1746,  0.3987, -0.5655],\n",
       "        ...,\n",
       "        [-0.1890, -0.1230,  0.3380,  ..., -0.1634,  0.6574, -0.3483],\n",
       "        [-0.1171,  0.0681,  0.2051,  ...,  0.1395,  0.2031, -0.1296],\n",
       "        [-0.0622, -0.1456,  0.4096,  ..., -0.1914,  0.2268, -0.1482]],\n",
       "       device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Don't use CLS and the SEP token \n",
    "# Watch out if this could be also done to ignore padding...\n",
    "hidden[:, 1:-1].sum(dim=1) / (hidden.shape[1] - 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
