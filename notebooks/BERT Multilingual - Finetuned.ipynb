{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Multilingual BERT + Fine tuning\n",
    "\n",
    "Let's first train a model in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "# Trying to cut this down to check if this improves memory usage\n",
    "max_input_length = 128 #tokenizer.max_len_single_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length]\n",
    "    return tokens\n",
    "\n",
    "TEXT = data.Field(\n",
    "    #tokenize = 'spacy', \n",
    "    #tokenizer_language=\"es\",\n",
    "    tokenize=tokenize_and_cut,\n",
    "    include_lengths = True,\n",
    "    use_vocab=False,\n",
    "    batch_first = True,\n",
    "    preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "    init_token = init_token_idx,\n",
    "    eos_token = eos_token_idx,\n",
    "    pad_token = pad_token_idx,\n",
    "    unk_token = unk_token_idx\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train instances: 90754\n",
      "Dev   instances: 13240\n"
     ]
    }
   ],
   "source": [
    "ID = data.Field(sequential=False, use_vocab=False)\n",
    "# All these arguments are because these are really floats\n",
    "# See https://github.com/pytorch/text/issues/78#issuecomment-541203609\n",
    "AVG = data.LabelField(dtype = torch.float, use_vocab=False, preprocessing=float)\n",
    "STD = data.LabelField(dtype = torch.float, use_vocab=False, preprocessing=float)\n",
    "SUBTASK_A = data.LabelField()\n",
    "\n",
    "train_dataset = data.TabularDataset(\n",
    "    \"../data/English/task_a_distant.sample.tsv\",\n",
    "    format=\"tsv\", skip_header=True,\n",
    "    fields=[(\"id\", ID), (\"text\", TEXT), (\"avg\", AVG), (\"std\", STD)],\n",
    ")\n",
    "\n",
    "dev_dataset = data.TabularDataset(\n",
    "    \"../data/olid/olid-training-v1.0.tsv\",\n",
    "    format=\"tsv\", skip_header=True,\n",
    "    fields=[(\"id\", ID), (\"text\", TEXT), \n",
    "            (\"subtask_a\", SUBTASK_A), (\"subtask_b\", None), (\"subtask_c\", None)],\n",
    "    \n",
    ")\n",
    "\n",
    "print(f\"Train instances: {len(train_dataset)}\")\n",
    "\n",
    "print(f\"Dev   instances: {len(dev_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vocabulary for label field. Can we just say 0 -> NOT (Offensive) 1 -> OFF? \n",
    "\n",
    "I don't know, just to make sure add the assertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBTASK_A.build_vocab(dev_dataset)\n",
    "\n",
    "assert SUBTASK_A.vocab.itos == [\"NOT\", \"OFF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¬ø', 'lo', 'creer', '##as', ',', 'aria', '##dna', '?', 'dijo', 'tese', '##o', '.', 'el', 'mino', '##tau', '##ro', 'apenas', 'se', 'defend', '##io', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 21, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"¬øLo creer√°s, Ariadna? dijo Teseo. El minotauro apenas se defendi√≥.\")\n",
    "\n",
    "token_ids = torch.LongTensor(tokenizer.convert_tokens_to_ids(tokens))\n",
    "# I need to reshape it before BERT consumes it\n",
    "# (batch_len, seq_len)... in this case batch_len == 1\n",
    "print(tokenizer.convert_ids_to_tokens(token_ids))\n",
    "last_hidden, clf = bert(token_ids.view(1, -1))\n",
    "\n",
    "last_hidden.shape, clf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers=1, \n",
    "                 bidirectional=False,\n",
    "                 finetune_bert=False,\n",
    "                 dropout=0.2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.finetune_bert = finetune_bert\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "    \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):    \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        if not self.finetune_bert:\n",
    "            with torch.no_grad():\n",
    "                embedded = self.bert(text)[0]\n",
    "        else:\n",
    "            embedded = self.bert(text)[0]\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output\n",
    "\n",
    "    #for name, param in model.named_parameters():                \n",
    "#    if name.startswith('bert'):\n",
    "#        param.requires_grad = False\n",
    "        \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_it, dev_it = data.BucketIterator.splits(\n",
    "    (train_dataset, dev_dataset), batch_size=BATCH_SIZE, device=device,\n",
    "    sort_key = lambda x: len(x.text), sort_within_batch = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, scheduler, max_grad_norm=None):\n",
    "    \"\"\"\n",
    "    Trains the model for one full epoch\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    is_tqdm = type(pbar).__module__.split(\".\")[0] == \"tqdm\"\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        text, lens = batch.text\n",
    "\n",
    "        predictions = model(text)\n",
    "        \n",
    "        target = 1.0 * (batch.avg > 0.6) \n",
    "        \n",
    "        loss = criterion(predictions.squeeze(1), target)\n",
    "        \n",
    "        prob_predictions = torch.sigmoid(predictions)\n",
    "        preds = torch.round(prob_predictions).detach().cpu()\n",
    "        acc = accuracy_score(preds, target.cpu())\n",
    "\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        if max_grad_norm:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        if is_tqdm:\n",
    "            desc = f\"Loss {epoch_loss / (i+1):.3f} -- Acc {epoch_acc / (i+1):.3f} -- LR {lr:.5f}\"\n",
    "            iterator.set_description(desc)\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given iterator\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_probas = []\n",
    "        labels = []\n",
    "        for batch in iterator:\n",
    "            text, lens = batch.text\n",
    "            target = batch.subtask_a\n",
    "            \n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions.squeeze(1), target.float())\n",
    "            \n",
    "            prob_predictions = torch.sigmoid(predictions)\n",
    "\n",
    "            predicted_probas.append(prob_predictions)\n",
    "            labels.append(target.cpu())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        predicted_probas = torch.cat(predicted_probas).cpu()\n",
    "        labels = torch.cat(labels).cpu()\n",
    "\n",
    "        preds = torch.round(predicted_probas)\n",
    "\n",
    "        pos_f1 = f1_score(labels, preds)\n",
    "        neg_f1 = f1_score(1-labels, 1-preds)\n",
    "        avg_f1 = (pos_f1 + neg_f1) / 2\n",
    "        acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return epoch_loss / len(iterator), acc, avg_f1, pos_f1, neg_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the class weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0090909090909093"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import compute_class_weight, compute_sample_weight\n",
    "y = []\n",
    "for elem in dev_dataset:\n",
    "    y.append(1*(elem.subtask_a == 'OFF'))\n",
    "    \n",
    "class_weights = compute_class_weight('balanced', [0, 1], y)\n",
    "\n",
    "# normalize it\n",
    "class_weights = class_weights / class_weights[0]\n",
    "\n",
    "class_weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 168,144,641 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.25\n",
    "\n",
    "\n",
    "model = BERTGRUSentiment(\n",
    "    bert,\n",
    "    finetune_bert=True,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([class_weights[1]]))\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8c955e62bf4438a5f7f200687d8e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5.0), HTML(value='')), layout=Layout(disp‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5abedc0b8174734825badd31f35e72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=2837.0), HTML(value='')), layout=Layout(d‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: Loss: 0.592 Acc: 85.57%\n",
      "Val. Loss: 1.008 Acc: 33.24% Macro F1 0.250 (P 0.499 - N 0.000)\n",
      "Best model so far (Loss 1.008 - Acc 0.332, F1 0.250) saved at /tmp/bert_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5684ba07e347d3ad784229ae8f75b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=2837.0), HTML(value='')), layout=Layout(d‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: Loss: 0.225 Acc: 95.53%\n",
      "Val. Loss: 1.405 Acc: 78.16% Macro F1 0.723 (P 0.595 - N 0.851)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96caffca765a4e2dbb5cf091feee0e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=2837.0), HTML(value='')), layout=Layout(d‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: Loss: 0.146 Acc: 96.69%\n",
      "Val. Loss: 1.572 Acc: 78.63% Macro F1 0.722 (P 0.587 - N 0.856)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f1b5567a734bae8e18354877d60709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=2837.0), HTML(value='')), layout=Layout(d‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: Loss: 0.124 Acc: 97.22%\n",
      "Val. Loss: 1.480 Acc: 79.71% Macro F1 0.748 (P 0.637 - N 0.859)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5be0bd3d02d48f6b2754027d5aa9c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=2837.0), HTML(value='')), layout=Layout(d‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: Loss: 0.113 Acc: 97.62%\n",
      "Val. Loss: 1.851 Acc: 79.92% Macro F1 0.749 (P 0.638 - N 0.861)\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from transformers.optimization import get_constant_schedule_with_warmup\n",
    "import time\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "# Copied from transformers page...please check\n",
    "num_training_steps = N_EPOCHS * len(train_it)\n",
    "num_warmup_steps = len(train_it)\n",
    "warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1\n",
    "\n",
    "scheduler = get_constant_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=num_warmup_steps, \n",
    ") \n",
    "\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "early_stopping_tolerance = 4\n",
    "epochs_without_improvement = 0\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "model_path = f\"/tmp/bert_model.pt\"\n",
    "\n",
    "pbar = tqdm(range(N_EPOCHS), ncols=1000)\n",
    "for epoch in pbar:\n",
    "\n",
    "    epoch_bar = tqdm(train_it, ncols=800)\n",
    "\n",
    "    train_loss, train_acc = train(\n",
    "        model, epoch_bar, optimizer, criterion,\n",
    "        max_grad_norm=max_grad_norm, scheduler=scheduler\n",
    "    )\n",
    "    valid_loss, valid_acc, valid_f1, pos_f1, neg_f1 = evaluate(model, dev_it, criterion)\n",
    "    \n",
    "    desc = f'Train: Loss: {train_loss:.3f} Acc: {train_acc*100:.2f}%'\n",
    "    desc += f'\\nVal. Loss: {valid_loss:.3f} Acc: {valid_acc*100:.2f}% Macro F1 {valid_f1:.3f} (P {pos_f1:.3f} - N {neg_f1:.3f})'\n",
    "    pbar.set_description(desc)\n",
    "    print(desc)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Best model so far (Loss {best_valid_loss:.3f} - Acc {valid_acc:.3f}, F1 {valid_f1:.3f}) saved at {model_path}\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= early_stopping_tolerance:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.851  Acc: 79.92% Macro F1: 0.749 Pos F1 0.638 Neg F1 0.861\n"
     ]
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "loss, acc, f1, pos_f1, neg_f1 = evaluate(model, dev_it, criterion)\n",
    "\n",
    "print(f'Val Loss: {loss:.3f}  Acc: {acc*100:.2f}% Macro F1: {f1:.3f} Pos F1 {pos_f1:.3f} Neg F1 {neg_f1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../models/bert.en-sample.ft.mean06.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "I reload the dataframe because it is not possible to retrieve the original text from the examples :-\\ (they are already tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet', 'subtask_a', 'subtask_b', 'subtask_c'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev = pd.read_table(\"../data/olid/olid-training-v1.0.tsv\", index_col=0)\n",
    "\n",
    "df_dev.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995aece011f341d1a9a536724f9b0099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=414.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "rows = []\n",
    "for batch in tqdm(dev_it):\n",
    "    text, lens = batch.text\n",
    "    \n",
    "    outs = model(text).detach().cpu()\n",
    "    ids = batch.id\n",
    "    labels = batch.subtask_a\n",
    "    probs = torch.sigmoid(outs)\n",
    "    preds = torch.round(probs)\n",
    "    \n",
    "    for tid, label, pred, prob in zip(ids.cpu(), labels.cpu(), preds.cpu(), probs.cpu()):\n",
    "        rows.append({\n",
    "            \"id\": tid.item(),\n",
    "            \"text\": df_dev.loc[tid.item(), \"tweet\"],\n",
    "            \"label\": label.item(),\n",
    "            \"pred\": int(pred.item()),\n",
    "            \"prob\": prob.item()\n",
    "        })\n",
    "        \n",
    "df = pd.DataFrame(rows)\n",
    "df.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falsos negativos: 2059\n",
      "Falsos positivos: 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred not off</th>\n",
       "      <th>pred off</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>not off</th>\n",
       "      <td>8240</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>off</th>\n",
       "      <td>2059</td>\n",
       "      <td>2341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pred not off  pred off\n",
       "real                           \n",
       "not off          8240       600\n",
       "off              2059      2341"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pos = df[(df[\"label\"] == 1) & (df[\"pred\"] == 1)].copy()\n",
    "false_neg = df[(df[\"label\"] == 1) & (df[\"pred\"] == 0)].copy()\n",
    "false_neg.sort_values(\"prob\", ascending=True, inplace=True)\n",
    "false_pos = df[(df[\"label\"] == 0) & (df[\"pred\"] == 1)].copy()\n",
    "false_pos.sort_values(\"prob\", ascending=False, inplace=True)\n",
    "true_neg = df[(df[\"label\"] == 0) & (df[\"pred\"] == 0)].copy()\n",
    "\n",
    "conf_matrix = pd.DataFrame([\n",
    "    {\"real\":\"not off\", \"pred not off\": len(true_neg),      \"pred off\": len(false_pos)},\n",
    "    {\"real\":\"off\",     \"pred not off\":     len(false_neg), \"pred off\": len(true_pos)}\n",
    "])\n",
    "\n",
    "conf_matrix.set_index(\"real\", inplace=True)\n",
    "\n",
    "print(\"Falsos negativos: {}\".format(len(false_neg)))\n",
    "print(\"Falsos positivos: {}\".format(len(false_pos)))\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred not off</th>\n",
       "      <th>pred off</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>not off</th>\n",
       "      <td>0.622</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>off</th>\n",
       "      <td>0.156</td>\n",
       "      <td>0.177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pred not off  pred off\n",
       "real                           \n",
       "not off         0.622     0.045\n",
       "off             0.156     0.177"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "conf_matrix / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit better recall!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred not off</th>\n",
       "      <th>pred off</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>not off</th>\n",
       "      <td>0.622</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>off</th>\n",
       "      <td>0.156</td>\n",
       "      <td>0.177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pred not off  pred off\n",
       "real                           \n",
       "not off         0.622     0.045\n",
       "off             0.156     0.177"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "conf_matrix / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terrible recall for offensive tweets. We've got to improve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@USER #StrataData #AI #data #hairball: @USER TDavis touring the application of #Cognitive and #machinelearning techniques to speed #Governance of #data URL'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_neg.iloc[0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this really offensive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     @USER #StrataData #AI #data #hairball: @USER T...\n",
       "label                                                    1\n",
       "pred                                                     0\n",
       "prob                                                 0.001\n",
       "Name: 22728, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_neg.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet        @USER I have you and Kaze- I need more.\n",
       "subtask_a                                        OFF\n",
       "subtask_b                                        TIN\n",
       "subtask_c                                        IND\n",
       "Name: 79079, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.loc[79079]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22728</th>\n",
       "      <td>@USER #StrataData #AI #data #hairball: @USER TDavis touring the application of #Cognitive and #machinelearning techniques to speed #Governance of #data URL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63235</th>\n",
       "      <td>#HanoiJane Fonda Campaigns for Gun Control Measure; Will it Backfire? -   Liberty Park Press URL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21970</th>\n",
       "      <td>Judicial Watch Sues for Records of FBI 302 Interviews with Demoted Justice Department Official Bruce Ohr\"  URL #TCOT #MAGA #RedNationRising\"</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45266</th>\n",
       "      <td>@USER @USER True but the constant talk from the left about common sense gun control every time and yet not one valid proposal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30890</th>\n",
       "      <td>#VA10 voters know @USER is the right side of the gun issue. Gun control or gun rights? The answer may help determine whether NRA-financed Rep. Comstock wins reelection URL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28037</th>\n",
       "      <td>Thinking about the Student-Loan Crisis\"  URL #TCOT #MAGA #RedNationRising\"</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59539</th>\n",
       "      <td>@USER @USER in every conner coz anti pakistani will not able to attack on our future generation rights</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38764</th>\n",
       "      <td>@USER @USER The right has the unique ability to create monsters out of thin air. It Legislation was even created to fight the phantom menance of antifa--currently in committee.  URL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65778</th>\n",
       "      <td>@USER @USER Indeed Americans need more gun control which means hitting your target. Can never have to much range time. ‚ò∫üòä</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12084</th>\n",
       "      <td>@USER Over 30 years worth of background checks under different Presidents and this one slipped through? #KavanaughConfirmation #KavanaughWithdraw #KavanaughSCOTUS #maga</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94226</th>\n",
       "      <td>@USER @USER He‚Äôs made a gun control shift as well. Check his talking points from last cycle.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19126</th>\n",
       "      <td>6 Reasons Your Right-Wing Friend Isn‚Äôt Budging On Gun Control URL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10256</th>\n",
       "      <td>Agreed great idea!  Wish we could also remove her from existence!  #MAGA #PatriotsUnited #ProLife #ProIsrael #WeAreQ. URL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93870</th>\n",
       "      <td>Understand the meaning of America by reading Creating the Declaration of Independence by @USER and share your knowledge of the American Creed with your fellow citizens. URL #MAGA #KAG @USER @USER @USER @USER</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87207</th>\n",
       "      <td>@USER @USER Quite possible. You are very right to be skeptical. But one has to start somewhere !</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95262</th>\n",
       "      <td>@USER Great Gun Control! Takes Concentration and Steady Hand! Way to Go Girl! URL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75090</th>\n",
       "      <td>@USER It's scary stuff to see how dedicated you are to preventing people's access to resources during a crisis.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14370</th>\n",
       "      <td>@USER Spoiler Alert : This episode has a hidden agenda to promote gun control</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79226</th>\n",
       "      <td>@USER Really? Can you show me the studies from countries that have common sense gun control to support your statement. Thx.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63195</th>\n",
       "      <td>@USER @USER Obama done nothing fix the problem while in office?  Now going about telling stories. Trump have being able to restore hope on gun control</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53883</th>\n",
       "      <td>6 more white boys standing up for you every day Ms. .@USER  I think the poster adds speshul emphasis. Don‚Äôt you?  #MAGA #WWG1WGA URL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31527</th>\n",
       "      <td>@USER He is quite good at faking. Must have done it numerous times</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78807</th>\n",
       "      <td>@USER @USER @USER @USER time for MAGA to flip it! ü§ó</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23152</th>\n",
       "      <td>@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER Followed most patriots already and got the rest appreciate a follow back #MAGA üëç</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15568</th>\n",
       "      <td>üá∫üá∏  FOR SALE  -  BUY #American #Flag #USA   ‚ñ∂  Shopping-Sale .org  ‚óÄ   #USA_Flag_4U #Shop_8_9 #MAGA #AmericaFirst @USER #Trump #POL_8_27_QX URL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60513</th>\n",
       "      <td>@USER Looks Like The Jokes On Liberals Again.  #FortTrump #Poland #BoomingEconomy URL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98013</th>\n",
       "      <td>@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER Retweet to the end of time. MAGA Trump2020 and beyond.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78604</th>\n",
       "      <td>@USER @USER  was spreading #FakeNews about @USER FYI. #MAGA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65110</th>\n",
       "      <td>@USER @USER @USER @USER You lied you don‚Äôt know about the Mulford Act.    URL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98230</th>\n",
       "      <td>.@USER @USER @USER @USER @USER @USER @USER @USER   And Liberals think GUNS are the problem? URL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                  text  \\\n",
       "id                                                                                                                                                                                                                       \n",
       "22728                                                      @USER #StrataData #AI #data #hairball: @USER TDavis touring the application of #Cognitive and #machinelearning techniques to speed #Governance of #data URL   \n",
       "63235                                                                                                                 #HanoiJane Fonda Campaigns for Gun Control Measure; Will it Backfire? -   Liberty Park Press URL   \n",
       "21970                                                                     Judicial Watch Sues for Records of FBI 302 Interviews with Demoted Justice Department Official Bruce Ohr\"  URL #TCOT #MAGA #RedNationRising\"   \n",
       "45266                                                                                    @USER @USER True but the constant talk from the left about common sense gun control every time and yet not one valid proposal   \n",
       "30890                                      #VA10 voters know @USER is the right side of the gun issue. Gun control or gun rights? The answer may help determine whether NRA-financed Rep. Comstock wins reelection URL   \n",
       "28037                                                                                                                                       Thinking about the Student-Loan Crisis\"  URL #TCOT #MAGA #RedNationRising\"   \n",
       "59539                                                                                                           @USER @USER in every conner coz anti pakistani will not able to attack on our future generation rights   \n",
       "38764                            @USER @USER The right has the unique ability to create monsters out of thin air. It Legislation was even created to fight the phantom menance of antifa--currently in committee.  URL   \n",
       "65778                                                                                        @USER @USER Indeed Americans need more gun control which means hitting your target. Can never have to much range time. ‚ò∫üòä   \n",
       "12084                                         @USER Over 30 years worth of background checks under different Presidents and this one slipped through? #KavanaughConfirmation #KavanaughWithdraw #KavanaughSCOTUS #maga   \n",
       "94226                                                                                                                     @USER @USER He‚Äôs made a gun control shift as well. Check his talking points from last cycle.   \n",
       "19126                                                                                                                                                6 Reasons Your Right-Wing Friend Isn‚Äôt Budging On Gun Control URL   \n",
       "10256                                                                                        Agreed great idea!  Wish we could also remove her from existence!  #MAGA #PatriotsUnited #ProLife #ProIsrael #WeAreQ. URL   \n",
       "93870  Understand the meaning of America by reading Creating the Declaration of Independence by @USER and share your knowledge of the American Creed with your fellow citizens. URL #MAGA #KAG @USER @USER @USER @USER   \n",
       "87207                                                                                                                 @USER @USER Quite possible. You are very right to be skeptical. But one has to start somewhere !   \n",
       "95262                                                                                                                                @USER Great Gun Control! Takes Concentration and Steady Hand! Way to Go Girl! URL   \n",
       "75090                                                                                                  @USER It's scary stuff to see how dedicated you are to preventing people's access to resources during a crisis.   \n",
       "14370                                                                                                                                    @USER Spoiler Alert : This episode has a hidden agenda to promote gun control   \n",
       "79226                                                                                      @USER Really? Can you show me the studies from countries that have common sense gun control to support your statement. Thx.   \n",
       "63195                                                           @USER @USER Obama done nothing fix the problem while in office?  Now going about telling stories. Trump have being able to restore hope on gun control   \n",
       "53883                                                                             6 more white boys standing up for you every day Ms. .@USER  I think the poster adds speshul emphasis. Don‚Äôt you?  #MAGA #WWG1WGA URL   \n",
       "31527                                                                                                                                               @USER He is quite good at faking. Must have done it numerous times   \n",
       "78807                                                                                                                                                              @USER @USER @USER @USER time for MAGA to flip it! ü§ó   \n",
       "23152                           @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER Followed most patriots already and got the rest appreciate a follow back #MAGA üëç   \n",
       "15568                                                                  üá∫üá∏  FOR SALE  -  BUY #American #Flag #USA   ‚ñ∂  Shopping-Sale .org  ‚óÄ   #USA_Flag_4U #Shop_8_9 #MAGA #AmericaFirst @USER #Trump #POL_8_27_QX URL   \n",
       "60513                                                                                                                            @USER Looks Like The Jokes On Liberals Again.  #FortTrump #Poland #BoomingEconomy URL   \n",
       "98013                                                     @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER Retweet to the end of time. MAGA Trump2020 and beyond.   \n",
       "78604                                                                                                                                                      @USER @USER  was spreading #FakeNews about @USER FYI. #MAGA   \n",
       "65110                                                                                                                                    @USER @USER @USER @USER You lied you don‚Äôt know about the Mulford Act.    URL   \n",
       "98230                                                                                                                  .@USER @USER @USER @USER @USER @USER @USER @USER   And Liberals think GUNS are the problem? URL   \n",
       "\n",
       "       label  pred  prob  \n",
       "id                        \n",
       "22728      1     0 0.001  \n",
       "63235      1     0 0.001  \n",
       "21970      1     0 0.001  \n",
       "45266      1     0 0.001  \n",
       "30890      1     0 0.001  \n",
       "28037      1     0 0.001  \n",
       "59539      1     0 0.001  \n",
       "38764      1     0 0.001  \n",
       "65778      1     0 0.001  \n",
       "12084      1     0 0.001  \n",
       "94226      1     0 0.001  \n",
       "19126      1     0 0.001  \n",
       "10256      1     0 0.001  \n",
       "93870      1     0 0.001  \n",
       "87207      1     0 0.001  \n",
       "95262      1     0 0.001  \n",
       "75090      1     0 0.001  \n",
       "14370      1     0 0.001  \n",
       "79226      1     0 0.001  \n",
       "63195      1     0 0.001  \n",
       "53883      1     0 0.001  \n",
       "31527      1     0 0.001  \n",
       "78807      1     0 0.001  \n",
       "23152      1     0 0.001  \n",
       "15568      1     0 0.001  \n",
       "60513      1     0 0.001  \n",
       "98013      1     0 0.001  \n",
       "78604      1     0 0.001  \n",
       "65110      1     0 0.001  \n",
       "98230      1     0 0.001  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', None)\n",
    "false_neg.iloc[:30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
